{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65fc8c3a-2118-42d1-b07f-3571a0eacd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU device : 0\n"
     ]
    }
   ],
   "source": [
    "from train_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38869168-3ac0-47b5-a1a3-3f8109100392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully loaded bert-base-uncased with BertTokenizer, BertForSequenceClassification\n"
     ]
    }
   ],
   "source": [
    "path_1 = \"res_models/IMDB/bertbase/output_N_pro/checkpoint-2000\"\n",
    "path_2 = \"res_models/IMDB/bertbase/output_N_pro/aa_early_stopping_5/checkpoint-3500\"\n",
    "\n",
    "tokenizet = load_hf('bertbase', load_model=False) #, path_to_model=path_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1dd8807a-b2f3-4832-a9b1-d0250dee852e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file res_models/IMDB/bertbase/output_N_pro/aa_early_stopping_5/checkpoint-3500/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file res_models/IMDB/bertbase/output_N_pro/aa_early_stopping_5/checkpoint-3500/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at res_models/IMDB/bertbase/output_N_pro/aa_early_stopping_5/checkpoint-3500.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(path_2, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "608e5ec3-97e2-4337-8aeb-12244f94c8d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.bert.modeling_bert.BertForSequenceClassification"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71e26d45-8827-4634-9f8b-2d62d3c23eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['res_models/IMDB/bertbase/output_N_pro/aa_early_stopping_5/checkpoint-3500']\n",
      "#################\n",
      "res_models/IMDB/bertbase/output_N_pro/aa_early_stopping_5/checkpoint-3500\n",
      "res_models/IMDB/bertbase/output_N_pro/aa_early_stopping_5/checkpoint-3500\n",
      "successfully loaded bert-base-uncased with BertTokenizer, BertForSequenceClassification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "filenames = glob(path_ + '*')\n",
    "filenames.reverse()\n",
    "print(filenames)\n",
    "print('#################')\n",
    "print(filenames[0])    \n",
    "model_path = filenames[0]\n",
    "print(model_path)\n",
    "model = load_hf('bertbase', path_2) # DistilBertForSequenceClassification.from_pretrained(, num_labels=2) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb4bf0d2-ba41-4eb4-9468-e63163e711d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01_10_1823calculate accuracy with RESTRICTED test_set\n",
      "01_10_1823calculate accuracy with ALL test samples\n"
     ]
    }
   ],
   "source": [
    "print(timestamp(True) + 'calculate accuracy with RESTRICTED test_set')\n",
    "test_data_res = pd.read_pickle('res_data/IMDB_training/IMDB_N_pro_test')\n",
    "\n",
    "print(timestamp(True)+ 'calculate accuracy with ALL test samples')\n",
    "test_data_all = pd.read_pickle('res_data/IMDB_l_test')\n",
    "\n",
    "\n",
    "test_data_res.label = pd.factorize(test_data_res.label)[0]\n",
    "test_data_all.label = pd.factorize(test_data_all.label)[0]\n",
    "if 'text' in test_data_res.columns[1]:\n",
    "    test_data_res.rename(columns={test_data_res.columns[1]:'text'}, inplace=True)\n",
    "else:\n",
    "    print(\"ERROR: compute_metrics - wrong column renamed. This is not the text column\")\n",
    "\n",
    "if 'text' in test_data_all.columns[1]:\n",
    "    test_data_all.rename(columns={test_data_all.columns[1]:'text'}, inplace=True)\n",
    "else:\n",
    "    print(\"ERROR: compute_metrics - wrong column renamed. This is not the text column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2508ed9e-d9ff-41a7-aa53-5e9c083b593e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=tokenizet[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2aa55251-d64e-4992-84b5-22266b884558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.bert.modeling_bert.BertForSequenceClassification"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b277c07-9d7a-4ed7-9fff-121c497aed62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PreTrainedTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=False, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}),\n",
       " None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7b40951-5d5d-4aa8-a718-982599c20bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 25000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1563' max='1563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1563/1563 04:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_130364/1029134316.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mraw_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mreturn_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mraw_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_data' is not defined"
     ]
    }
   ],
   "source": [
    "# ----- Predict -----#\n",
    "X_test = list(test_data_res[\"text\"])\n",
    "X_test_tokenized = tokenizer(X_test, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Create torch dataset\n",
    "test_dataset = Dataset(X_test_tokenized)\n",
    "\n",
    "# Define test trainer\n",
    "test_trainer = Trainer(model)\n",
    "\n",
    "# Make prediction\n",
    "raw_pred, _, _ = test_trainer.predict(test_dataset)\n",
    "\n",
    "return_res = compute_metrics([raw_pred,list(test_data_res[\"label\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "135ac264-a4ad-431f-9a60-a219d5fec2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01_10_1843 : compute_metrics - accuracy: 0.9358; precision: 0.9387130546830957; recall: 0.93248; f1: 0.9355861460047357\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9358,\n",
       " 'precision': 0.9387130546830957,\n",
       " 'recall': 0.93248,\n",
       " 'f1': 0.9355861460047357}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_res = compute_metrics([raw_pred,list(test_data_res[\"label\"])])\n",
    "return_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "67998124-ea48-4b16-9794-2268b8add8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 25000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1563' max='1563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1563/1563 04:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01_10_1849 : compute_metrics - accuracy: 0.93612; precision: 0.9399564199822452; recall: 0.93176; f1: 0.935840263549074\n"
     ]
    }
   ],
   "source": [
    "# ----- Predict -----#\n",
    "X_test = list(test_data_all[\"text\"])\n",
    "X_test_tokenized = tokenizer(X_test, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Create torch dataset\n",
    "test_dataset = Dataset(X_test_tokenized)\n",
    "\n",
    "# Define test trainer\n",
    "test_trainer = Trainer(model)\n",
    "\n",
    "# Make prediction\n",
    "raw_pred, _, _ = test_trainer.predict(test_dataset)\n",
    "\n",
    "return_all = compute_metrics([raw_pred,list(test_data_all[\"label\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3e17d8b8-a6cd-4a6f-8bc2-e82b34a2709d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.93612,\n",
       " 'precision': 0.9399564199822452,\n",
       " 'recall': 0.93176,\n",
       " 'f1': 0.935840263549074}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_all"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c2891cd5-1557-4fac-8b70-2312cf77985e",
   "metadata": {},
   "source": [
    "es_models/IMDB/bertbase/output_N_pro/checkpoint-2000\n",
    "res_models/IMDB/bertbase/output_N_pro/checkpoint-2000\n",
    "successfully loaded bert-base-uncased with BertTokenizer, BertForSequenceClassification\n",
    "01_10_0441calculate accuracy with ALL test samples\n",
    "01_10_0446 : compute_metrics - accuracy: 0.92712; precision: 0.9180238020670216; recall: 0.938; f1: 0.9279044001266222\n",
    "   accuracy  precision   recall        f1 data set   spec\n",
    "0   0.92612   0.916165  0.93808  0.926993     spec  N_pro\n",
    "1   0.92712   0.918024  0.93800  0.927904      all  N_pro\n",
    "res_models/IMDB/bertbase/output_N_weat/\n",
    "['res_models/IMDB/bertbase/output_N_weat/checkpoint-3000', 'res_models/IMDB/bertbase/output_N_weat/checkpoint-2500', 'res_models/IMDB/bertbase/output_N_weat/checkpoint-2000', 'res_models/IMDB/bertbase/output_N_weat/checkpoint-1500', 'res_models/IMDB/bertbase/output_N_weat/checkpoint-1000', 'res_models/IMDB/bertbase/output_N_weat/checkpoint-500', 'res_models/IMDB/bertbase/output_N_weat/runs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dfcd95-c30f-4402-800a-69ea35c9a5cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91165703-1d0e-4f5f-8376-aa8105a57aae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e951f064-a787-4d0e-98b5-cdfb9b44ff49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11792ba5-5d22-4481-b1b0-3f3451aa0b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_acc(spec, tokenizer, model_id, task=\"foo\",  restricted_test_set = False):\n",
    "    # ----- Load trained model -----#   \n",
    "    path_ = \"res_models/IMDB/bertbase/output_N_pro/aa_early_stopping_5/checkpoint-3500\"\n",
    "    print(path_)\n",
    "    #filenames = next(walk(path_), (None, [], None))[1]  # [] if no file\n",
    "    filenames = glob(path_ + '*')\n",
    "    filenames.reverse()\n",
    "    print(filenames)\n",
    "    print('#################')\n",
    "    print(filenames[0])    \n",
    "    model_path = filenames[0]\n",
    "    print(model_path)\n",
    "    model = load_hf(model_id, path_to_model=model_path) # DistilBertForSequenceClassification.from_pretrained(, num_labels=2) \n",
    "    \n",
    "    # ----- Load test data -----#\n",
    "    if restricted_test_set: \n",
    "        print(timestamp(True) + 'calculate accuracy with RESTRICTED test_set')\n",
    "        test_data = pd.read_pickle('res_data/{}_training/{}_{}_test'.format(task,task,spec))\n",
    "    else:\n",
    "        print(timestamp(True)+ 'calculate accuracy with ALL test samples')\n",
    "        test_data = pd.read_pickle('res_data/' + task + '_l_test')\n",
    "    test_data.label = pd.factorize(test_data.label)[0]\n",
    "    if 'text' in test_data.columns[1]:\n",
    "        test_data.rename(columns={test_data.columns[1]:'text'}, inplace=True)\n",
    "    else:\n",
    "        print(\"ERROR: compute_metrics - wrong column renamed. This is not the text column\")\n",
    "        # log.error(__name__ + \": compute_metrics - \" + 'wrong column renamed. This is not the text column')          \n",
    "        \n",
    "    # ----- Predict -----#\n",
    "    X_test = list(test_data[\"text\"])\n",
    "    X_test_tokenized = tokenizer(X_test, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "    # Create torch dataset\n",
    "    test_dataset = Dataset(X_test_tokenized)\n",
    "\n",
    "    # Define test trainer\n",
    "    test_trainer = Trainer(model)\n",
    "\n",
    "    # Make prediction\n",
    "    raw_pred, _, _ = test_trainer.predict(test_dataset)\n",
    "    \n",
    "    return(compute_metrics([raw_pred,list(test_data[\"label\"])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54ec9288-43a7-42bb-8216-116844a1a452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res_models/IMDB/bertbase/output_N_pro/aa_early_stopping_5/checkpoint-3500\n"
     ]
    }
   ],
   "source": [
    "path_ = \"res_models/IMDB/bertbase/output_N_pro/aa_early_stopping_5/checkpoint-3500\"\n",
    "print(path_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11dc9607-2e5a-44e0-9277-4d4163411d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "file res_models/IMDB/bertbase/output_N_pro/aa_early_stopping_5/config.json not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res_models/IMDB/bertbase/output_N_pro/\n",
      "['res_models/IMDB/bertbase/output_N_pro/aa_early_stopping_5', 'res_models/IMDB/bertbase/output_N_pro/checkpoint-2000', 'res_models/IMDB/bertbase/output_N_pro/checkpoint-1500', 'res_models/IMDB/bertbase/output_N_pro/checkpoint-1000', 'res_models/IMDB/bertbase/output_N_pro/checkpoint-500', 'res_models/IMDB/bertbase/output_N_pro/runs']\n",
      "#################\n",
      "res_models/IMDB/bertbase/output_N_pro/aa_early_stopping_5\n",
      "res_models/IMDB/bertbase/output_N_pro/aa_early_stopping_5\n",
      "successfully loaded bert-base-uncased with BertTokenizer, BertForSequenceClassification\n",
      "res_models/IMDB/bertbase/output_N_pro/aa_early_stopping_5\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load config for 'res_models/IMDB/bertbase/output_N_pro/aa_early_stopping_5'. Make sure that:\n\n- 'res_models/IMDB/bertbase/output_N_pro/aa_early_stopping_5' is a correct model identifier listed on 'https://huggingface.co/models'\n  (make sure 'res_models/IMDB/bertbase/output_N_pro/aa_early_stopping_5' is not a path to a local directory with something else, in that case)\n\n- or 'res_models/IMDB/bertbase/output_N_pro/aa_early_stopping_5' is the correct path to a directory containing a config.json file\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/localdata2/jent_so/miniforge3/envs/tensorflow/lib/python3.9/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             resolved_config_file = cached_path(\n\u001b[0m\u001b[1;32m    551\u001b[0m                 \u001b[0mconfig_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/localdata2/jent_so/miniforge3/envs/tensorflow/lib/python3.9/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;31m# File, but it doesn't exist.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1506\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"file {url_or_filename} not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1507\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: file res_models/IMDB/bertbase/output_N_pro/aa_early_stopping_5/config.json not found",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_130364/4293909313.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0macc_rest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"N_pro\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bertbase'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"IMDB\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestricted_test_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/localdata2/jent_so/LM_GenderBias/bias-bert/train_util.py\u001b[0m in \u001b[0;36mcalc_acc\u001b[0;34m(spec, tokenizer, model_id, task, restricted_test_set, log)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_hf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# DistilBertForSequenceClassification.from_pretrained(, num_labels=2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;31m# ----- Load test data -----#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/localdata2/jent_so/LM_GenderBias/bias-bert/train_util.py\u001b[0m in \u001b[0;36mload_hf\u001b[0;34m(model_id, load_model, path_to_model)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_to_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Function should end here. Something is going wrong'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmodel_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"bertlarge\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/localdata2/jent_so/miniforge3/envs/tensorflow/lib/python3.9/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1257\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m             \u001b[0mconfig_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1259\u001b[0;31m             config, model_kwargs = cls.config_class.from_pretrained(\n\u001b[0m\u001b[1;32m   1260\u001b[0m                 \u001b[0mconfig_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m                 \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/localdata2/jent_so/miniforge3/envs/tensorflow/lib/python3.9/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \"\"\"\n\u001b[0;32m--> 493\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             logger.warn(\n",
      "\u001b[0;32m/localdata2/jent_so/miniforge3/envs/tensorflow/lib/python3.9/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"- or '{revision}' is a valid git identifier (branch name, a tag name, or a commit id) that exists for this model name as listed on its model page on 'https://huggingface.co/models'\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnicodeDecodeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load config for 'res_models/IMDB/bertbase/output_N_pro/aa_early_stopping_5'. Make sure that:\n\n- 'res_models/IMDB/bertbase/output_N_pro/aa_early_stopping_5' is a correct model identifier listed on 'https://huggingface.co/models'\n  (make sure 'res_models/IMDB/bertbase/output_N_pro/aa_early_stopping_5' is not a path to a local directory with something else, in that case)\n\n- or 'res_models/IMDB/bertbase/output_N_pro/aa_early_stopping_5' is the correct path to a directory containing a config.json file\n\n"
     ]
    }
   ],
   "source": [
    "acc_rest = calc_acc(\"N_pro\", tokenizet, 'bertbase', task=\"IMDB\", restricted_test_set = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caf7d0f-093e-4597-956c-2da9dba0225e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
